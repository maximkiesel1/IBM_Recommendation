{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "In this notebook, you will be putting your recommendation skills to use on real data from the IBM Watson Studio platform. \n",
    "\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/rubrics/3325/view).  **Please save regularly.**\n",
    "\n",
    "By following the table of contents, you will build out a number of different methods for making recommendations that can be used for different situations. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations (EXTRA - NOT REQUIRED)](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)\n",
    "\n",
    "At the end of the notebook, you will find directions for how to submit your work.  Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>b96a4f2e92d8572034b1e9b28f9ac673765cd074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>06485706b34a5c9bf2a0ecdac41daf7e7654ceb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>f01220c46fc92c6e6b161b1849de11faacd7ccb2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0      1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1      1314.0       healthcare python streaming application demo   \n",
       "2      1429.0         use deep learning for image classification   \n",
       "3      1338.0          ml optimization using cognitive assistant   \n",
       "4      1276.0          deploy your python model as a restful api   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  \n",
       "2  b96a4f2e92d8572034b1e9b28f9ac673765cd074  \n",
       "3  06485706b34a5c9bf2a0ecdac41daf7e7654ceb7  \n",
       "4  f01220c46fc92c6e6b161b1849de11faacd7ccb2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import project_tests as t\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "df_content = pd.read_csv('data/articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to get an idea of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of...</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video demonstrates the power of IBM DataS...</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_body  \\\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "2  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "3  DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...   \n",
       "4  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "\n",
       "                                     doc_description  \\\n",
       "0  Detect bad readings in real time using Python ...   \n",
       "1  See the forest, see the trees. Here lies the c...   \n",
       "2  Here’s this week’s news in Data Science and Bi...   \n",
       "3  Learn how distributed DBs solve the problem of...   \n",
       "4  This video demonstrates the power of IBM DataS...   \n",
       "\n",
       "                                       doc_full_name doc_status  article_id  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1  Communicating data science: A guide to present...       Live           1  \n",
       "2         This Week in Data Science (April 18, 2017)       Live           2  \n",
       "3  DataLayer Conference: Boost the performance of...       Live           3  \n",
       "4      Analyze NY Restaurant data using Spark in DSX       Live           4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show df_content to get an idea of the data\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "Use the dictionary and cells below to provide some insight into the descriptive statistics of the data.\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?  Provide a visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the appearances of each unique value in each column\n",
    "dict_multi_val = {}\n",
    "for column in list(df.columns):\n",
    "    dict_multi_val[column] = {}\n",
    "    if len(df[column].unique()) > 1 and len(df[column].unique()) < df.shape[0]:\n",
    "        for unique_value in df[column].unique():\n",
    "            dict_multi_val[column][unique_value] = len(df[df[column]==unique_value].index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the appearances of all unique values in 'email'\n",
    "num_viewed_artic = dict_multi_val['email'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max value in num_viewed_artic\n",
    "max_value = max(num_viewed_artic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2341cbea58>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show histogram of the multi viewed articles\n",
    "\n",
    "# define the plt size\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# x label\n",
    "plt.xlabel('Number Of Viewed Articles')\n",
    "\n",
    "# y label\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# title\n",
    "plt.title('Histogram: Viewed Articles')\n",
    "\n",
    "# change the segmentation\n",
    "plt.xticks(list(range(0, 360, 10)))\n",
    "\n",
    "# Create histogram\n",
    "plt.hist(num_viewed_artic, bins=20)\n",
    "\n",
    "# Get histogram counts and bins\n",
    "counts, bins = np.histogram(list(num_viewed_artic), bins=20)\n",
    "\n",
    "# Compute bin widths and centers\n",
    "widths = bins[1:] - bins[:-1]\n",
    "centers = (bins[1:] + bins[:-1]) / 2\n",
    "\n",
    "# Compute percentages\n",
    "percents = counts / len(num_viewed_artic) * 100\n",
    "\n",
    "# Add percentages to the histogram\n",
    "for i, percent in enumerate(percents):\n",
    "    plt.text(centers[i], counts[i], f\"{percent:.1f}%\", ha='center', va='bottom')\n",
    "    \n",
    "# show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the median and maximum number of user_article interactions below\n",
    "\n",
    "median_val = statistics.median(num_viewed_artic) # 50% of individuals interact with ____ number of articles or fewer.\n",
    "max_views_by_user = max_value # The maximum number of user-article interactions by any 1 user is ______."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Explore and remove duplicate articles from the **df_content** dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all values in the categorical columns, which appears more more the once\n",
    "# count the number of them\n",
    "article_multi_val = {}\n",
    "for column in list(df_content.columns):\n",
    "    article_multi_val[column] = {}\n",
    "    if len(df_content[column].unique()) > 1 and len(df_content[column].unique()) < df_content.shape[0]:\n",
    "        for unique_value in df_content[column].unique():\n",
    "            if len(df_content[df_content[column]==unique_value].index) > 1:\n",
    "                article_multi_val[column][unique_value] = len(df_content[df_content[column]==unique_value].index)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "50\n",
      "-------------\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            doc_body  \\\n",
      "50   Follow Sign in / Sign up Home About Insight Data Science Data Engineering Health Data AI Never miss a story from Insight Data , when you sign up for Medium. Learn more Never miss a story from Insight Data Get updates Get updates Sebastien Dery Blocked Unblock Follow Following I don’t know what I’m doing; but then neither do you so it’s all good. Master\\r\\nof Layers, Protector of the Graph, Wielder of Knowledge. #OpenScience Oct 16\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nGRAPH-BASED MACHINE LEARNING: PART I\\r\\nCOMMUNITY DETECTION AT SCALE\\r\\nDuring the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large, real-time datasets.\\r\\n\\r\\nSebastien Dery (now a Data Science Engineer at Yewno ) discusses his project on community detection on large datasets.\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\n#tltr : Graph-based machine learning is a powerful tool that can easily be merged\\r\\ninto ongoing efforts. Using modularity as an optimization goal provides a\\r\\nprincipled approach to community detection. Local modularity increment can be\\r\\ntweaked to your own dataset to reflect interpretable quantities. This is useful\\r\\nin many scenarios, making it a prime candidate for your everyday toolbox.Many important problems can be represented and studied using graphs — social\\r\\nnetworks, interacting bacterias, brain network modules, hierarchical image\\r\\nclustering and many more.\\r\\n\\r\\nIf we accept graphs as a basic means of structuring and analyzing data about the\\r\\nworld, we shouldn’t be surprised to see them being widely used in Machine\\r\\nLearning as a powerful tool that can enable intuitive properties and power a lot\\r\\nof useful features. Graph-based machine learning is destined to become a\\r\\nresilient piece of logic, transcending a lot of other techniques. See more in\\r\\nthis recent blog post from Google Research\\r\\n\\r\\nThis post explores the tendencies of nodes in a graph to spontaneously form\\r\\nclusters of internally dense linkage (hereby termed “community”); a remarkable\\r\\nand almost universal property of biological networks. This is particularly\\r\\ninteresting knowing that a lot of information can be extrapolated from a node’s\\r\\nneighbor (e.g. think recommendation system, respondent analysis, portfolio\\r\\nclustering). So how can we extract this kind of information?\\r\\n\\r\\nCommunity Detection aims to partition a graph into clusters of densely connected nodes, with the\\r\\nnodes belonging to different communities being only sparsely connected.\\r\\n\\r\\nGraph analytics concerns itself with the study of nodes (depicted as disks) and\\r\\ntheir interactions with other nodes (lines). Community Detection aims to\\r\\nclassify nodes by their “clique”.“ Is it the same as clustering? ”\\r\\n\\r\\n * Short answer: Yes .\\r\\n * Long answer: For all intents and purposes, yes it is .\\r\\n\\r\\nSo why shouldn’t I just use my good old K-Means? You absolutely should, unless\\r\\nyour data and requirements don’t work well with that algorithm’s assumptions,\\r\\nnamely:\\r\\n\\r\\n 1. K number of clusters\\r\\n 2. Sum of Squared Error (SSE) as the right optimization cost\\r\\n 3. All variable have the same variance\\r\\n 4. The variance of the distribution of each attribute is spherical\\r\\n\\r\\nFor a more in-depth look click here .\\r\\n\\r\\nFirst off, let’s drop this idea of SSE and choose a more relevant notation of\\r\\nwhat we’re looking for: the internal versus external relationships between nodes\\r\\nof a community. Let’s discuss the notion of modularity.\\r\\n\\r\\nwhere: nc is the number of communities; lc number of edges within; dc sum of vertex degree; and m the size of the graph (number of edges). We will be using this equation as a\\r\\nglobal metric of goodness during our search for an optimal partitioning. In a nutshell: Higher score will be given to a community configuration offering higher\\r\\ninternal versus external linkage.So all I have to do is optimize this and we’re done, right?\\r\\n\\r\\nA major problem in the theoretical formulation of this optimization scheme is\\r\\nthat we need an all-knowing knowledge of the graph topology (geometric\\r\\nproperties and spatial relations). This is rather, let’s say, intractable . Apparently we can’t do any better than to try all possible subsets of the\\r\\nvertices and check to see which, if any, form communities. The problem of finding the largest clique in a graph is thus said to be NP-hard .\\r\\n\\r\\nHowever, several algorithms have been proposed over the years to find reasonably good partitions in reasonable amounts of time, each with its own particular flavor. This post focuses on a\\r\\nspecific family of algorithms called agglomerative . These algorithms work very simply by collecting (or merging) nodes together.\\r\\nThis has a lot of advantages since it typically only requires a knowledge of first degree neighbors and small incremental merging steps , to bring the global solution towards stepwise equilibriums.\\r\\n\\r\\nYou might point out that the modularity metric gives a global perspective on the\\r\\nstate of the graph and not a local indicator. So, how does this translate to the\\r\\nsmall local increment that I just mentioned?\\r\\n\\r\\nThe basic approach does indeed consists of iteratively merging nodes that\\r\\noptimize a local modularity so let’s go ahead and define that as well:\\r\\n\\r\\nwhere ∑ in is the sum of weighted links inside C, ∑ tot sum of weighted links incident to nodes in C, k i sum of weighted links incident to node i , k i, in sum of weighted links going from i to nodes in C and m a normalizing factor as the sum of weighted links for the whole graph. (Sorry, Medium doesn’t allow subscript and superscript)This is part of the magic for me as this local optimization function can easily\\r\\nbe translated to an interpretable metric within the domain of your graph. For\\r\\nexample,\\r\\n\\r\\n * Community Strength: Sum of Weighted Link within a community.\\r\\n * Community Popularity: Sum of Weighted Link incident to nodes within a specific community.\\r\\n * Node Belonging: Sum of Weighted Link from a node to a community.\\r\\n\\r\\nThere’s also nothing stopping from adding more terms to the previous equation\\r\\nthat are specific to your dataset. In other words, the weighted links can be a\\r\\nfunction of the type of nodes computed on-the-fly (useful if you’re dealing with\\r\\na multidimensional graph with various types of relationships and nodes).\\r\\n\\r\\nExample of converging iterations before the Compress phaseNow that we’re all set with our optimization function and local cost, the\\r\\ntypical agglomerative strategy consists of two iterative phases ( Transfer and Compress ). Assuming a weighted network of N nodes, we begin by assigning a different community to each node of the network.\\r\\n\\r\\n 1. Transfer : For each node i, consider its neighbors j and evaluate the gain in modularity by swapping c_i for c_j . The greedy process transfers the node into the neighboring community,\\r\\n    maximizing the gain in modularity (assuming the gain is positive). If no\\r\\n    positive gain is possible, the node i stays in its original community. This process is applied to all nodes until\\r\\n    no individual move can improve the modularity (i.e. a local maxima of\\r\\n    modularity is attained — a state of equilibrium).\\r\\n 2. Compress : building a new network whose nodes are the communities found during the\\r\\n    first phase; a process termed compression (see Figure below). To do so, edge weights between communities are computed\\r\\n    as the sum of the internal edges between nodes in the corresponding two\\r\\n    communities.\\r\\n\\r\\nAgglomerative process: Phase one converges to a local equilibrium of local\\r\\nmodularity. Phase two consist in compressing the graph for the next iteration,\\r\\nthus reducing the number of nodes to consider and incidentally computation time\\r\\nas well.Now the tricky part: as this is a greedy algorithm , you’ll have to define a stopping criteria based on your case scenario and the\\r\\ndata at hand.\\r\\n\\r\\nHow to define this criteria? It can be a lot of things: a maximum number of iterations, a minimum modularity\\r\\ngain during the transfer phase, or any other relevant piece of information\\r\\nrelated to your data that would inform you that it needs to stop.\\r\\n\\r\\nStill not sure when to stop ? Just make sure you save every intermediate step of the iterative process\\r\\nsomewhere, let the optimization run until there’s only one node left in your\\r\\ngraph, and then look back at your data! The interesting part is that by keeping\\r\\ntrack of each step, you also profit from a hierarchical view of your communities\\r\\nwhich can be further explored and leveraged.\\r\\n\\r\\nIn a follow up post, I will discuss how we can achieve this on a distributed\\r\\nsystem using Spark GraphX , part of my project while at the Insight Data Engineering Fellows Program .\\r\\n\\r\\n[0803.0476] Fast unfolding of communities in large networks Abstract: We propose\\r\\na simple method to extract the community structure of large networks. Our method\\r\\nis a heuristic… arxiv.org\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nWant to learn Spark, machine learning with graphs, and other big data tools from\\r\\ntop data engineers in Silicon Valley or New York? The Insight Data Engineering Fellows Program is a free 7-week professional training where you can build cutting edge big\\r\\ndata platforms and transition to a career in data engineering at top teams like\\r\\nFacebook, Uber, Slack and Squarespace.\\r\\n\\r\\nLearn more about the program and apply today .\\r\\n\\r\\nBig Data Data Science Machine Learning Social Network Analysis Insight Data Engineering 4 Blocked Unblock Follow FollowingSEBASTIEN DERY\\r\\nI don’t know what I’m doing; but then neither do you so it’s all good. Mast...   \n",
      "365  Follow Sign in / Sign up Home About Insight Data Science Data Engineering Health Data AI 5 * Share\\r\\n * 5\\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from Insight Data , when you sign up for Medium. Learn more Never miss a story from Insight Data Get updates Get updates Sebastien Dery Blocked Unblock Follow Following Master of Layers, Protector of the Graph, Wielder of Knowledge. #OpenScience\\r\\n#NoBullshit 2 days ago\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nGRAPH-BASED MACHINE LEARNING: PART 2\\r\\nCOMMUNITY DETECTION AT SCALE\\r\\nDuring the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large, real-time datasets.\\r\\n\\r\\nSebastien Dery (now a Data Science Engineer at Yewno ) discusses his project on community detection on large datasets.\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\n#tltr : Graph-based machine learning is a powerful tool that can easily be merged\\r\\ninto ongoing efforts. This work reviews the feasibility of performing community\\r\\ndetection through a distributed implementation using GraphX. Embedded within the\\r\\nHadoop ecosystem, this modularity optimization approach allows the study of\\r\\nnetworks of unprecedented size. This change of scales, previously limited by\\r\\nRAM, opens exciting perspectives as the self modular structure of complex\\r\\nsystems have been shown to hold crucial information to understanding their\\r\\nnature.In my previous post , we discussed the foundation of community detection using modularity\\r\\noptimization. One major constraint however, is that your graph needs to fit in\\r\\nmemory. This quickly turns problematic as your number of nodes surpass billions, and\\r\\nthe number of edges becomes trillions.\\r\\n\\r\\nThankfully we can leverage distributed computation systems in order to solve this limitation. To do this we first need to define the state\\r\\nof a node so that it contains all the information needed during computation;\\r\\nthis will serve as a basic structure to pass around between the machines of our\\r\\ndistributed cluster.\\r\\n\\r\\n“Node” and “Vertex” are often used interchangeably in the literature. This class\\r\\nserves as structure for the nodes within the graph.Let’s also briefly review the process behind modularity optimization. This works\\r\\nby iteratively merging nodes that optimize for local modularity to yield a new, and smaller, graph. Repeat until satisfied.\\r\\n\\r\\nTwo great properties emerge from this approach\\r\\n\\r\\n 1. Locality : Each node requires knowledge from only its first-degree neighbors. This\\r\\n    means a minimal amount of data needs to be passed around between clusters.\\r\\n    This way, you don’t need to extensively jump from node to node across the\\r\\n    clusters in order to get the necessary information.\\r\\n 2. Independence : Each local computation occurs independently of the graph layout. Within\\r\\n    an iteration, every node can asynchronously send its information to its\\r\\n    neighbors without waiting for a blocking sequential set of operations to\\r\\n    happen.\\r\\n\\r\\nThese are important points to highlight as they make distributed computation a\\r\\nprime candidate for this memory problem. Turns out we can easily implement the\\r\\nlogic behind those properties using nothing but a simple iteration and a\\r\\ndeveloper-defined halting criteria. As previously discussed this can take many forms; here are a few ideas for brainstorming:\\r\\n\\r\\n * Scheduled based on a predefined number of iterations\\r\\n * Hits a specific total number of communities\\r\\n * Modularity gain between iteration is below a threshold\\r\\n\\r\\nSimple iteration over the two stage process of our optimization: transfer and\\r\\ncompress.Let’s dive into the initial step of transferring community between nodes.\\r\\nRemember that each node needs the information from its neighbors in order to\\r\\ncompute the gradient for local modularity.\\r\\n\\r\\nTRANSFER\\r\\nThe best way to do this at scale (when you don’t know where the information\\r\\nultimately is on disk) is by using distributed transactions (aka passing messages ). This type of architecture is ubiquitous in modern computer software; it is\\r\\nused as a way for the objects that make up a program to work with each other and\\r\\nas a way for objects and systems running on different computers (e.g., the\\r\\nInternet) to interact. In algorithms, you’ll often find it referenced under the\\r\\nname of Belief Propagation or simply message passing . In the context of community detection, each node sends a message to its\\r\\nneighbors with content along the lines of:\\r\\n\\r\\n“ Hey I’m your friendly neighbor Node 3 from Community 12 ”\\r\\n\\r\\nBy independently sending messages to their first degree neighbors, each node can\\r\\nretrieve all the information necessary for them to optimize for local\\r\\nmodularity. The content of each message can easily be tweaked thus adding\\r\\nconsiderable flexibility to your approach.If you’ve ever worked with graphs you’re likely to be very familiar with the\\r\\nconcepts of vertices and edges . Should we perform the message passing exhaustively you’d basically go through\\r\\neach vertex and send a message for each of its edges. This is not an\\r\\nintrinsically bad approach if that’s all you have to work with. Turns out that\\r\\nin the world of GraphX we have access to a third primitive for easy manipulation of our data: the triplet .\\r\\n\\r\\nThe three different types of view allowed within GraphX. Taken from AMPLab .The triplet logically joins the vertex and edge properties for a simplified and\\r\\nuseful view. Literally, the EdgeTriplet class extends the Edge class by simply adding the srcAttr and dstAttr members containing the source and destination properties respectively.\\r\\n\\r\\nBy reducing the triplets view, each node receives N messages corresponding to its N first degree neighbors. sendMsg and mergeMsg are both internal functions which perform the necessary aggregation for the\\r\\nlocal modularity update. Independently, and in parallel, each node waits for its\\r\\nturn to reduce all its messages into a coherent local sum of weighted edges, and\\r\\nmake a decision based on the local modularity deltaQ of each neighboring community.\\r\\n\\r\\nA few iterations later, the graph has converged to a local equilibrium (e.g. a\\r\\nminimal amount of nodes feel the need to change community). The algorithm can\\r\\nnow progress to the next step of compressing those communities into a compact\\r\\nrepresentation. This is done by creating a new graph with a new set of nodes\\r\\n(corresponding to each community) and edges being inferred from the edges during\\r\\nthe previous computation (e.g. average or sum of external edges).\\r\\n\\r\\nCOMPRESSION\\r\\nWhat function to choose really depends on the use case (e.g. averaging, total\\r\\nsum, maximum, softmax , etc. are all valid functions, although their respective advantages remains\\r\\nunclear in this particular scenario). When in doubt, let’s use a simple average.\\r\\nNote that additional information, say the internal coherence within a community,\\r\\ncan be propagated in a similar fashion to the condensed node and provide\\r\\nvaluable information.\\r\\n\\r\\nEffect of compressing community into single nodes at each iteration.Finally, here we have a fully functional procedure to perform modularity\\r\\noptimization on graphs of ridiculously large size, assuming we have enough\\r\\ncomputers to store all the information on disk.\\r\\n\\r\\nCAVEATS AND TIPS\\r\\nCOMPUTATION TIME\\r\\nNote that the number of meta-communities naturally decreases at each pass, and\\r\\nas a consequence most of the computing time is used in the first pass. This\\r\\nsuggests pre-ordering of the data would hold considerable benefit in terms of\\r\\ncomputation time.\\r\\n\\r\\nOptimizing for node locality at the cluster level means less transfer between\\r\\nmachines.CONVERGENCE\\r\\nThis approach does not necessarily converge to the optimal solution . To improve this, multiple iterations can increase confidence over the\\r\\nstructure of your data. Conveniently, this also offers a proxy for the\\r\\nprobability of two nodes belonging to the same community.\\r\\n\\r\\nLAYOUT\\r\\nTake into account graph connectivity when determining the usefulness of this\\r\\nstrategy. For example, for a completely connected and unweighted graph, the\\r\\noutput will be degenerate. Consider thresholding the graph beforehand to extract\\r\\na more sparse representation of your data.\\r\\n\\r\\nThe adequateness of modularity optimization is dependent on the connectivity\\r\\npattern of your graph. For example, in a lattice layout this algorithm will\\r\\nperform rather poorly. Modularity optimization doesn’t guarantee adequate\\r\\nclustering; thus obtaining a community at the end is not enough to conclusively\\r\\nsay a node decidedly belongs to that group (or even any group, for that matter).HIERARCHY\\r\\nThe iterative nature of this process offers a hierarchical view between\\r\\ncommunities of subsequent iteration. The intermediary step should therefore be\\r\\nsaved for further investigation as they likely yield valuable information on the\\r\\nstructural complexity of the data. This saving procedure is not covered in this\\r\\npost but should be trivial to introduce (insert configuration state into your\\r\\nfavorite database) between iteration.\\r\\n\\r\\nSUMMARY\\r\\nThis work reviewed the feasibility of performing community detection through a\\r\\ndistributed implementation using GraphX . Embedded within the Hadoop ecosystem , this modularity optimization approach allows the study of networks of\\r\\nunprecedented size. This change of scales, previously limited by RAM, opens\\r\\nexciting perspectives as the self modular structure of complex systems have been\\r\\nshown to hold crucial information to understanding their nature. This enables,\\r\\namong others, targeted marketing , market seg...   \n",
      "\n",
      "                                                                                                                                                                                           doc_description  \\\n",
      "50                                                                                                                                                                            Community Detection at Scale   \n",
      "365  During the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large…   \n",
      "\n",
      "                    doc_full_name doc_status  article_id  \n",
      "50   Graph-based machine learning       Live          50  \n",
      "365  Graph-based machine learning       Live          50  \n",
      "-------------\n",
      "221\n",
      "-------------\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            doc_body  \\\n",
      "221  * United States\\r\\n\\r\\nIBM® * Site map\\r\\n\\r\\nSearch within Bluemix Blog Bluemix Blog * About Bluemix * What is Bluemix\\r\\n    * Getting Started\\r\\n    * Case Studies\\r\\n    * Hybrid Architecture\\r\\n    * Open Source\\r\\n    * Trust, Security, Privacy\\r\\n    * Data Centers\\r\\n    * Our Network\\r\\n    * Automation\\r\\n    * Architecture Center\\r\\n   \\r\\n   \\r\\n * Products * Compute Infrastructure\\r\\n    * Compute Services\\r\\n    * Hybrid Deployments\\r\\n    * Watson\\r\\n    * Internet of Things\\r\\n    * Mobile\\r\\n    * DevOps\\r\\n    * Data Analytics\\r\\n    * Network\\r\\n    * Open Source\\r\\n    * Storage\\r\\n    * Security\\r\\n   \\r\\n   \\r\\n * Services * Bluemix Services\\r\\n    * Garage\\r\\n   \\r\\n   \\r\\n * Pricing\\r\\n * Support * Support\\r\\n    * Contact Us\\r\\n    * Resources\\r\\n    * Docs\\r\\n   \\r\\n   \\r\\n * Blog * How-tos\\r\\n    * Trending\\r\\n    * What's New\\r\\n    * Events\\r\\n   \\r\\n   \\r\\n * Partners * Partners\\r\\n    * Become a Partner\\r\\n    * Find a Partner\\r\\n   \\r\\n   \\r\\n * Sign up\\r\\n\\r\\nDATA ANALYTICSHOW SMART CATALOGS CAN TURN THE BIG DATA FLOOD INTO AN OCEAN OF OPPORTUNITY\\r\\nAugust 1, 2017 | Written by: Jay Limburn\\r\\n\\r\\nCategorized: Data Analytics\\r\\n\\r\\nShare this post:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nOne of the earliest documented catalogs was compiled at the great library of\\r\\nAlexandria in the third century BC, to help scholars manage, understand and\\r\\naccess its vast collection of literature. While that cataloging process\\r\\nrepresented a massive undertaking for the Alexandrian librarians, it pales in\\r\\ncomparison to the task of wrangling the volume and variety of data that modern\\r\\norganizations generate.\\r\\n\\r\\nNowadays, data is often described as an organization’s most valuable asset, but\\r\\nunless users can easily sift through data artifacts to find the information they\\r\\nneed, the value of that data may remain unrealized. Catalogs can solve this\\r\\nproblem by providing an indexed set of information about the organization’s\\r\\ndata, storing metadata that describes all assets and providing a reference to\\r\\nwhere they can be found or accessed.\\r\\n\\r\\nIt’s not just the size and complexity of the data that makes cataloging a tough\\r\\nchallenge: organizations also need to be able to perform increasingly\\r\\ncomplicated operations on that data at high speed, and even in real-time. As a\\r\\nresult, technology leaders must continually find better ways to solve today’s\\r\\nversion of the same cataloging challenges faced in Alexandria all those years\\r\\nago.\\r\\n\\r\\n\\r\\n\\r\\nENTER IBM\\r\\nIBM’s aim with Watson Data Platform is to make data accessible for anyone who uses it. An integral part of Watson\\r\\nData Platform will be a new intelligent asset catalog, IBM Data Manager, a\\r\\nsolution underpinned by a central repository of metadata describing all the\\r\\ninformation managed by the platform. Unlike many other catalog solutions on the\\r\\nmarket, the intelligent asset catalog will also offer full end-to-end\\r\\ncapabilities around data lifecycle and governance.\\r\\n\\r\\nBecause all the elements of Watson Data Platform can utilize the same catalog,\\r\\nusers will be able to share data with their colleagues more easily, regardless\\r\\nof what the data is, where it is stored, or how they intend to use it. In this\\r\\nway, the intelligent asset catalog will unlock the value held within that data\\r\\nacross user groups—helping organizations use this key asset to its full\\r\\npotential.\\r\\n\\r\\n\\r\\n\\r\\nBREAKING DOWN SILOS\\r\\nWith Watson Data Platform, data engineers, data scientists and other knowledge\\r\\nworkers throughout an enterprise can search for, share and leverage assets\\r\\n(including datasets, files, connections, notebooks, data flows, models and\\r\\nmore). Assets can be accessed using the Data Science Experience web user interface to analyze data,\\r\\n\\r\\nTo collaborate with colleagues, users can put assets into a Project that acts as\\r\\na shared sandbox where the whole team can access and utilize them. Once their\\r\\nwork is complete, they can submit any resulting content to the catalog for\\r\\nfurther reuse by other people and groups across the organization.\\r\\n\\r\\nRich metadata about each asset makes it easy for knowledge workers to find and\\r\\naccess relevant resources. Along with data files, the catalog can also include\\r\\nconnections to databases and other data sources, both on- and off-premises,\\r\\ngiving users a full 360-degree view to all information relevant to their\\r\\nbusiness, regardless of where or how it is stored.\\r\\n\\r\\n\\r\\n\\r\\nMANAGING DATA OVER TIME\\r\\nIt’s important to look at data as an evolving asset, rather than something that\\r\\nstays fixed over time. To help manage and trace this evolution, IBM Data Manager\\r\\nwill keep a complete track of which users have added or modified each asset, so\\r\\nthat it is always clear who is responsible for any changes.\\r\\n\\r\\n\\r\\n\\r\\nSMART CATALOG CAPABILITIES FOR BIG DATA MANAGEMENT\\r\\nThe concept of catalogs may be simple, but when they’re being used to make sense\\r\\nof huge amounts of constantly changing data, smart capabilities make all the\\r\\ndifference. Here are some of the key smart catalog functionalities that we see\\r\\nas integral to tackling the big data challenge, and that we will be aiming to\\r\\ninclude in upcoming releases of IBM Data Manager.\\r\\n\\r\\n\\r\\n\\r\\nDATA AND ASSET TYPE AWARENESS\\r\\nWhen a user chooses to preview or view an asset of a particular type, the data\\r\\nand asset type awareness feature will automatically launch the data in the best\\r\\nviewer—such as a shaper for a dataset, or a canvas for a data flow. This will\\r\\nsave time and boost productivity for users, optimizing discovery and making it\\r\\neasier to work with a variety of data types without switching tools.\\r\\n\\r\\n\\r\\n\\r\\nINTELLIGENT SEARCH AND EXPLORATION\\r\\nBy combining metadata, machine learning-based algorithms and user interaction\\r\\ndata, it is possible to fine-tune search results over time. Presenting users\\r\\nwith the most relevant data for their purpose will increase usefulness of the\\r\\nsolution the more it is used.\\r\\n\\r\\n\\r\\n\\r\\nSOCIAL CURATION\\r\\nEffective use of data throughout your organization is a two-way street: when\\r\\nusers discover a useful dataset, it’s important for them to help others find it\\r\\ntoo. Users can be encouraged to engage by taking advantage of curation features,\\r\\nenabling them to tag, rank and comment on assets within the catalog. By\\r\\naugmenting the metadata for each asset, this can help the catalog’s intelligent\\r\\nsearch algorithms guide users to the assets that are most relevant to their\\r\\nneeds.\\r\\n\\r\\n\\r\\n\\r\\nDATA LINEAGE\\r\\nIf data is incomplete or inaccurate, utilizing it can cause more problems than\\r\\nit solves. On the other hand, if data is accurate but users do not trust it,\\r\\nthey might not use it when it could make a real difference. In either scenario,\\r\\ndata lineage can help.\\r\\n\\r\\nData lineage captures the complete history of an asset in the catalog: from its\\r\\noriginal source, through all the operations and transformations it has\\r\\nundergone, to its current state. By exploring this lineage, users can be\\r\\nconfident they know where assets have come from, how those assets have evolved,\\r\\nand whether they can be trusted.\\r\\n\\r\\n\\r\\n\\r\\nMONITORING\\r\\nTaking a step back to a higher-level view, monitoring features will help users\\r\\nkeep track of overall usage of the catalog. Real-time dashboards help chief data\\r\\nofficers and other data professionals monitor how data is being used, and\\r\\nidentify ways to increase its usage in different areas of the organization.\\r\\n\\r\\n\\r\\n\\r\\nMETADATA DISCOVERY\\r\\nWe have already mentioned that data needs to be seen as an evolving asset—which\\r\\nmeans our catalogs must evolve with it. We plan to make it easy for users to\\r\\naugment assets with metadata manually; in the future, it may also be possible to\\r\\nintegrate algorithms that can discover assets and capture their metadata\\r\\nautomatically.\\r\\n\\r\\n\\r\\n\\r\\nDATA GOVERNANCE\\r\\nFor many organizations, keeping data secure while ensuring access for authorized\\r\\nusers is one of the most significant information management challenges. You can\\r\\nmitigate this challenge with rule-based access control and automatic enforcement\\r\\nof data governance policies.\\r\\n\\r\\n\\r\\n\\r\\nAPIS\\r\\nFinally, the catalog will enable access to all these capabilities and more\\r\\nthrough a set of well-defined, RESTful APIs. IBM is committed to offering\\r\\napplication developers easy access to additional components of Watson Data Platform , such as persistence stores and data sets. We hope that they can use our\\r\\nservices to extend their current suite of data and analytics tools, to innovate\\r\\nand create smart new ways of working with data.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nIn our next post, we’ll discuss the challenges around data governance, and\\r\\nexplore how IBM Data Manager can help you make light work of addressing them.\\r\\n\\r\\nJAY LIMBURN\\r\\nJay Limburn\\r\\n\\r\\nTHOMAS SCHAECK\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nPrevious Post\\r\\n\\r\\nWebSphere on the Cloud: Application Modernization (Phase 1)Next Post\\r\\n\\r\\nMaximize Control with IBM Bluemix Virtual serversADD COMMENT NO COMMENTS\\r\\nLEAVE A REPLY CANCEL REPLY\\r\\nYour email address will not be published. Required fields are marked *\\r\\n\\r\\nComment\\r\\n\\r\\nName *\\r\\n\\r\\nEmail *\\r\\n\\r\\nWebsite\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSearch for:RECENT POSTS\\r\\n * IBM Watson Machine Learning – General Availability\\r\\n * Locating IoT with Skyhook Precision Location\\r\\n * Monitoring IBM Bluemix Container Service with Sysdig Container Intelligence\\r\\n * Mobile Foundation Service integration with Mobile Analytics Service\\r\\n * Intel® Optane™ SSD DC P4800X Available Now on IBM Cloud\\r\\n\\r\\nARCHIVES\\r\\nArchives Select Month August 2017 July 2017 June 2017 May 2017 April 2017 March 2017 February 2017 January 2017 December 2016 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 March 2016 February 2016 Janu...   \n",
      "692                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Homepage Follow Sign in / Sign up Homepage * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nSusanna Tai Blocked Unblock Follow Following Offering Manager, Watson Data Platform | Data Catalog Oct 30\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nHOW SMART CATALOGS CAN TURN THE BIG DATA FLOOD INTO AN OCEAN OF OPPORTUNITY\\r\\nOne of the earliest documented catalogs was compiled at the great library of\\r\\nAlexandria in the third century BC, to help scholars manage, understand and\\r\\naccess its vast collection of literature. While that cataloging process\\r\\nrepresented a massive undertaking for the Alexandrian librarians, it pales in\\r\\ncomparison to the task of wrangling the volume and variety of data that modern\\r\\norganizations generate.\\r\\n\\r\\nNowadays, data is often described as an organization’s most valuable asset, but\\r\\nunless users can easily sift through data artifacts to find the information they\\r\\nneed, the value of that data may remain unrealized. Catalogs can solve this\\r\\nproblem by providing an indexed set of information about the organization’s\\r\\ndata, storing metadata that describes all assets and providing a reference to\\r\\nwhere they can be found or accessed.\\r\\n\\r\\nIt’s not just the size and complexity of the data that makes cataloging a tough\\r\\nchallenge: organizations also need to be able to perform increasingly\\r\\ncomplicated operations on that data at high speed, and even in real-time. As a\\r\\nresult, technology leaders must continually find better ways to solve today’s\\r\\nversion of the same cataloging challenges faced in Alexandria all those years\\r\\nago.\\r\\n\\r\\nENTER IBM\\r\\nIBM’s aim with Watson Data Platform is to make data accessible for anyone who uses it. An integral part of Watson\\r\\nData Platform will be a new intelligent asset catalog, IBM Data Catalog , a solution underpinned by a central repository of metadata describing all the\\r\\ninformation managed by the platform. Unlike many other catalog solutions on the\\r\\nmarket, the intelligent asset catalog will also offer full end-to-end\\r\\ncapabilities around data lifecycle and governance.\\r\\n\\r\\nBecause all the elements of Watson Data Platform can utilize the same catalog,\\r\\nusers will be able to share data with their colleagues more easily, regardless\\r\\nof what the data is, where it is stored, or how they intend to use it. In this\\r\\nway, the intelligent asset catalog will unlock the value held within that data\\r\\nacross user groups — helping organizations use this key asset to its full\\r\\npotential.\\r\\n\\r\\nBREAKING DOWN SILOS\\r\\nWith Watson Data Platform, data engineers, data scientists and other knowledge\\r\\nworkers throughout an enterprise can search for, share and leverage assets\\r\\n(including datasets, files, connections, notebooks, data flows, models and\\r\\nmore). Assets can be accessed using the Data Science Experience web user interface to analyze data,\\r\\n\\r\\nTo collaborate with colleagues, users can put assets into a Project that acts as\\r\\na shared sandbox where the whole team can access and utilize them. Once their\\r\\nwork is complete, they can submit any resulting content to the catalog for\\r\\nfurther reuse by other people and groups across the organization.\\r\\n\\r\\nRich metadata about each asset makes it easy for knowledge workers to find and\\r\\naccess relevant resources. Along with data files, the catalog can also include\\r\\nconnections to databases and other data sources, both on- and off-premises,\\r\\ngiving users a full 360-degree view to all information relevant to their\\r\\nbusiness, regardless of where or how it is stored.\\r\\n\\r\\nMANAGING DATA OVER TIME\\r\\nIt’s important to look at data as an evolving asset, rather than something that\\r\\nstays fixed over time. To help manage and trace this evolution, IBM Data Catalog\\r\\nwill keep a complete track of which users have added or modified each asset, so\\r\\nthat it is always clear who is responsible for any changes.\\r\\n\\r\\nSMART CATALOG CAPABILITIES FOR BIG DATA MANAGEMENT\\r\\nThe concept of catalogs may be simple, but when they’re being used to make sense\\r\\nof huge amounts of constantly changing data, smart capabilities make all the\\r\\ndifference. Here are some of the key smart catalog functionalities that we see\\r\\nas integral to tackling the big data challenge.\\r\\n\\r\\nDATA AND ASSET TYPE AWARENESS\\r\\nWhen a user chooses to preview or view an asset of a particular type, the data\\r\\nand asset type awareness feature will automatically launch the data in the best\\r\\nviewer — such as a shaper for a dataset, or a canvas for a data flow. This will\\r\\nsave time and boost productivity for users, optimizing discovery and making it\\r\\neasier to work with a variety of data types without switching tools.\\r\\n\\r\\nINTELLIGENT SEARCH AND EXPLORATION\\r\\nBy combining metadata, machine learning-based algorithms and user interaction\\r\\ndata, it is possible to fine-tune search results over time. Presenting users\\r\\nwith the most relevant data for their purpose will increase usefulness of the\\r\\nsolution the more it is used.\\r\\n\\r\\nSOCIAL CURATION\\r\\nEffective use of data throughout your organization is a two-way street: when\\r\\nusers discover a useful dataset, it’s important for them to help others find it\\r\\ntoo. Users can be encouraged to engage by taking advantage of curation features,\\r\\nenabling them to tag, rank and comment on assets within the catalog. By\\r\\naugmenting the metadata for each asset, this can help the catalog’s intelligent\\r\\nsearch algorithms guide users to the assets that are most relevant to their\\r\\nneeds.\\r\\n\\r\\nDATA LINEAGE\\r\\nIf data is incomplete or inaccurate, utilizing it can cause more problems than\\r\\nit solves. On the other hand, if data is accurate but users do not trust it,\\r\\nthey might not use it when it could make a real difference. In either scenario,\\r\\ndata lineage can help.\\r\\n\\r\\nData lineage captures the complete history of an asset in the catalog: from its\\r\\noriginal source, through all the operations and transformations it has\\r\\nundergone, to its current state. By exploring this lineage, users can be\\r\\nconfident they know where assets have come from, how those assets have evolved,\\r\\nand whether they can be trusted.\\r\\n\\r\\nMONITORING\\r\\nTaking a step back to a higher-level view, monitoring features will help users\\r\\nkeep track of overall usage of the catalog. Real-time dashboards help chief data\\r\\nofficers and other data professionals monitor how data is being used, and\\r\\nidentify ways to increase its usage in different areas of the organization.\\r\\n\\r\\nMETADATA DISCOVERY\\r\\nWe have already mentioned that data needs to be seen as an evolving asset —\\r\\nwhich means our catalogs must evolve with it. We plan to make it easy for users\\r\\nto augment assets with metadata manually; in the future, it may also be possible\\r\\nto integrate algorithms that can discover assets and capture their metadata\\r\\nautomatically.\\r\\n\\r\\nDATA GOVERNANCE\\r\\nFor many organizations, keeping data secure while ensuring access for authorized\\r\\nusers is one of the most significant information management challenges. You can\\r\\nmitigate this challenge with rule-based access control and automatic enforcement\\r\\nof data governance policies.\\r\\n\\r\\nAPIS\\r\\nFinally, the catalog will enable access to all these capabilities and more\\r\\nthrough a set of well-defined, RESTful APIs. IBM is committed to offering\\r\\napplication developers easy access to additional components of Watson Data Platform , such as persistence stores and data sets. We hope that they can use our\\r\\nservices to extend their current suite of data and analytics tools, to innovate\\r\\nand create smart new ways of working with the data.\\r\\n\\r\\nLearn more about IBM Data Catalog\\r\\n\\r\\n\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nWritten by Jay Limburn\\r\\nDistinguished Engineer and Offering Lead, Watson Data Platform\\r\\n\\r\\nOriginally published at www.ibm.com on August 1, 2017\\r\\n\\r\\n * Data Catalog\\r\\n * Data Management\\r\\n * Data Analytics\\r\\n * IBM\\r\\n * Ibm Watson\\r\\n\\r\\nA single golf clap? Or a long standing ovation?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\nBlocked Unblock Follow FollowingSUSANNA TAI\\r\\nOffering Manager, Watson Data Platform | Data Catalog\\r\\n\\r\\nFollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * \\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates   \n",
      "\n",
      "                                                                                                                                                                                        doc_description  \\\n",
      "221                                                                        When used to make sense of huge amounts of constantly changing data, smart catalog capabilities can make all the difference.   \n",
      "692  One of the earliest documented catalogs was compiled at the great library of Alexandria in the third century BC, to help scholars manage, understand and access its vast collection of literature…   \n",
      "\n",
      "                                                                   doc_full_name  \\\n",
      "221  How smart catalogs can turn the big data flood into an ocean of opportunity   \n",
      "692  How smart catalogs can turn the big data flood into an ocean of opportunity   \n",
      "\n",
      "    doc_status  article_id  \n",
      "221       Live         221  \n",
      "692       Live         221  \n",
      "-------------\n",
      "232\n",
      "-------------\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     doc_body  \\\n",
      "232  Homepage Follow Sign in Get started Homepage * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nCarmen Ruppach Blocked Unblock Follow Following Offering Manager for Data Refinery on Watson Data Platform at IBM Nov 14\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nSELF-SERVICE DATA PREPARATION WITH IBM DATA REFINERY\\r\\nIf you are like most data scientists, you are probably spending a lot of time to\\r\\ncleanse, shape and prepare your data before you can actually start with the more\\r\\nenjoyable part of building and training machine learning models. As a data\\r\\nanalyst, you might face similar struggles to obtain data in a format you need to\\r\\nbuild your reports. In many companies data scientists and analysts need to wait\\r\\nfor their IT teams to get access to cleaned data in a consumable format.\\r\\n\\r\\nIBM Data Refinery addresses this issue. It provides an intuitive self-service\\r\\ndata preparation environment where you can quickly analyze, cleanse and prepare\\r\\ndata sets. It is a fully managed cloud service, available in open beta now.\\r\\n\\r\\nAnalyze and prepare your data\\r\\n\\r\\nWith IBM Data Refinery, you can interactively explore your data and use a wide\\r\\nrange of transformations to cleanse and transform data into the format you need\\r\\nfor analysis.\\r\\n\\r\\nYou can use a simple point-and-click interface for selecting and combining a\\r\\nwide range of built-in operations, such as filtering, replacing, and deriving\\r\\nvalues. It is also possible to quickly remove duplicates, split and concatenate\\r\\nvalues, and choose from a comprehensive list of text and math operations.\\r\\n\\r\\nInteractive data exploration and preparationIf you prefer to code, in IBM Data Refinery you can directly enter R commands\\r\\nvia R libraries such as dplyr. We provide code templates and in-context\\r\\ndocumentation to help you become productive with the R syntax more quickly.\\r\\n\\r\\nCode templates to help users with R syntaxIf you’re not satisfied with the shaping results, you can easily undo and change\\r\\noperations in the Steps side bar.\\r\\n\\r\\nThe interactive user interface works on a subset of the data to give you a\\r\\nfaster preview of the operations and results. Once you’re happy with the sample\\r\\noutput, you can apply the transformations on the entire data set and save all\\r\\ntransformation steps in a data flow. You can repeat the data flow later and\\r\\ntrack changes that were applied to your data. To accelerate the job execution,\\r\\nApache Spark is used as the execution engine.\\r\\n\\r\\nData profiling and visualization\\r\\n\\r\\nData shaping is an iterative and time-consuming process. In a traditional data\\r\\nscience workflow, you might use one tool to apply various transformations to\\r\\nyour data set, and then load the data into another tool to visualize and\\r\\nevaluate the results. Over many cycles, this continual tool hopping can become\\r\\nfrustrating.\\r\\n\\r\\nIBM Data Refinery soothes the pain by integrating both data transformations and\\r\\nvisualizations in a single interface, so you can move between views with a\\r\\nsimple click. You can use the Profile tab to view descriptive statistics of your\\r\\ndata columns in order to better understand the distribution of values. You can\\r\\ncontinue to apply transformations and the corresponding profile information\\r\\nadjusts automatically.\\r\\n\\r\\nOn the Visualization tab you can select a combination of columns to build charts\\r\\nusing Brunel (open source visualization library). IBM Data Refinery\\r\\nautomatically suggests appropriate plots and you can choose between 12\\r\\npre-defined chart types. You can adjust the appearance of the charts using\\r\\nBrunel syntax.\\r\\n\\r\\nConnecting to data wherever it resides\\r\\n\\r\\nIBM Data Refinery comes with a comprehensive set of 30 prebuilt data connectors\\r\\nso that you can set up connections to a wide range of commonly used on-premises\\r\\nand cloud data stores. You can connect to IBM as well as non-IBM services. If\\r\\nyour data service is hosted on IBM Cloud (formerly IBM Bluemix), you can\\r\\ndirectly access the data service instance from IBM Data Refinery.\\r\\n\\r\\nOnce you specify a connection and connect the data object to your data, you can\\r\\nstart to analyze and refine your data wherever it resides.\\r\\n\\r\\nTry out IBM Data Refinery! Sign up for free at: https://www.ibm.com/cloud/data-refinery\\r\\n\\r\\n * Data Science\\r\\n * Data Visualization\\r\\n * Data Analysis\\r\\n * Data Refinery\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\nBlocked Unblock Follow FollowingCARMEN RUPPACH\\r\\nOffering Manager for Data Refinery on Watson Data Platform at IBM\\r\\n\\r\\nFollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * \\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data Platform , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Platform Get updates Get updates   \n",
      "971                               Homepage Follow Sign in Get started * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nCarmen Ruppach Blocked Unblock Follow Following Offering Manager for Data Refinery on Watson Data Platform at IBM Nov 14, 2017\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nSELF-SERVICE DATA PREPARATION WITH IBM DATA REFINERY\\r\\nIf you are like most data scientists, you are probably spending a lot of time to\\r\\ncleanse, shape and prepare your data before you can actually start with the more\\r\\nenjoyable part of building and training machine learning models. As a data\\r\\nanalyst, you might face similar struggles to obtain data in a format you need to\\r\\nbuild your reports. In many companies data scientists and analysts need to wait\\r\\nfor their IT teams to get access to cleaned data in a consumable format.\\r\\n\\r\\nIBM Data Refinery addresses this issue. It provides an intuitive self-service\\r\\ndata preparation environment where you can quickly analyze, cleanse and prepare\\r\\ndata sets. It is a fully managed cloud service, available in open beta now.\\r\\n\\r\\nAnalyze and prepare your data\\r\\n\\r\\nWith IBM Data Refinery, you can interactively explore your data and use a wide\\r\\nrange of transformations to cleanse and transform data into the format you need\\r\\nfor analysis.\\r\\n\\r\\nYou can use a simple point-and-click interface for selecting and combining a\\r\\nwide range of built-in operations, such as filtering, replacing, and deriving\\r\\nvalues. It is also possible to quickly remove duplicates, split and concatenate\\r\\nvalues, and choose from a comprehensive list of text and math operations.\\r\\n\\r\\nInteractive data exploration and preparationIf you prefer to code, in IBM Data Refinery you can directly enter R commands\\r\\nvia R libraries such as dplyr. We provide code templates and in-context\\r\\ndocumentation to help you become productive with the R syntax more quickly.\\r\\n\\r\\nCode templates to help users with R syntaxIf you’re not satisfied with the shaping results, you can easily undo and change\\r\\noperations in the Steps side bar.\\r\\n\\r\\nThe interactive user interface works on a subset of the data to give you a\\r\\nfaster preview of the operations and results. Once you’re happy with the sample\\r\\noutput, you can apply the transformations on the entire data set and save all\\r\\ntransformation steps in a data flow. You can repeat the data flow later and\\r\\ntrack changes that were applied to your data. To accelerate the job execution,\\r\\nApache Spark is used as the execution engine.\\r\\n\\r\\nProfile and visualize data\\r\\n\\r\\nData shaping is an iterative and time-consuming process. In a traditional data\\r\\nscience workflow, you might use one tool to apply various transformations to\\r\\nyour data set, and then load the data into another tool to visualize and\\r\\nevaluate the results. Over many cycles, this continual tool hopping can become\\r\\nfrustrating.\\r\\n\\r\\nIBM Data Refinery soothes the pain by integrating both data transformations and\\r\\nvisualizations in a single interface, so you can move between views with a\\r\\nsimple click. You can use the Profile tab to view descriptive statistics of your\\r\\ndata columns in order to better understand the distribution of values. You can\\r\\ncontinue to apply transformations and the corresponding profile information\\r\\nadjusts automatically.\\r\\n\\r\\nOn the Visualization tab you can select a combination of columns to build charts\\r\\nusing Brunel (open source visualization library). IBM Data Refinery\\r\\nautomatically suggests appropriate plots and you can choose between 12\\r\\npre-defined chart types. You can adjust the appearance of the charts using\\r\\nBrunel syntax.\\r\\n\\r\\nConnect to your data wherever it resides\\r\\n\\r\\nIBM Data Refinery comes with a comprehensive set of 30 prebuilt data connectors\\r\\nso that you can set up connections to a wide range of commonly used on-premises\\r\\nand cloud data stores. You can connect to IBM as well as non-IBM services. If\\r\\nyour data service is hosted on IBM Cloud (formerly IBM Bluemix), you can\\r\\ndirectly access the data service instance from IBM Data Refinery.\\r\\n\\r\\nOnce you specify a connection and connect the data object to your data, you can\\r\\nstart to analyze and refine your data wherever it resides.\\r\\n\\r\\nTry out IBM Data Refinery! Sign up for free at: https://www.ibm.com/cloud/data-refinery\\r\\n\\r\\n * Data Science\\r\\n * Data Visualization\\r\\n * Data Analysis\\r\\n * Data Refinery\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n27 Blocked Unblock Follow FollowingCARMEN RUPPACH\\r\\nOffering Manager for Data Refinery on Watson Data Platform at IBM\\r\\n\\r\\nFollowIBM WATSON DATA\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n\\r\\n * 27\\r\\n * \\r\\n * \\r\\n * \\r\\n\\r\\nNever miss a story from IBM Watson Data , when you sign up for Medium. Learn more Never miss a story from IBM Watson Data Get updates Get updates   \n",
      "\n",
      "                                                                                                                                                                                       doc_description  \\\n",
      "232  If you are like most data scientists, you are probably spending a lot of time to cleanse, shape and prepare your data before you can actually start with the more enjoyable part of building and…   \n",
      "971  If you are like most data scientists, you are probably spending a lot of time to cleanse, shape and prepare your data before you can actually start with the more enjoyable part of building and…   \n",
      "\n",
      "                                            doc_full_name doc_status  \\\n",
      "232  Self-service data preparation with IBM Data Refinery       Live   \n",
      "971  Self-service data preparation with IBM Data Refinery       Live   \n",
      "\n",
      "     article_id  \n",
      "232         232  \n",
      "971         232  \n",
      "-------------\n",
      "398\n",
      "-------------\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            doc_body  \\\n",
      "399  Homepage Follow Sign in Get started * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nSourav Mazumder Blocked Unblock Follow Following Nov 27\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nUSING APACHE SPARK AS A PARALLEL PROCESSING FRAMEWORK FOR ACCESSING REST BASED\\r\\nDATA SERVICES\\r\\nToday’s world of data science leverages data from various sources. Commonly,\\r\\nthese sources are Hadoop File System, Enterprise Data Warehouse, Relational\\r\\nDatabase systems, Enterprise file systems, etc. The data from these sources are\\r\\naccessed in bulk using connectors specific to the underlying technology and\\r\\noptimized for accessing large volume of data.\\r\\n\\r\\nHowever, many a times, a data science exploration/modeling exercise also needs\\r\\nto access data from sources that support only API-based data access. These\\r\\nAPI-based data sources/data services can be of various types. For example:\\r\\n\\r\\n * Data services (external or internal), which can provide curated/enriched data\\r\\n   in record-by-record manner.\\r\\n * Validation services for verifying the data using an API. For example Address\\r\\n   validation.\\r\\n * Machine learning/AI services, which provide prediction, recommendations, and\\r\\n   insights based on a single input record.\\r\\n * Service from internal systems (like CRM, MDM, etc.) of the organization,\\r\\n   which supports data access through API only in record-by-record manner.\\r\\n * And many more …\\r\\n\\r\\nThese API-based data services are commonly implemented using REST architectural\\r\\nstyle ( https://en.wikipedia.org/wiki/Representational_state_transfer ) and are designed to be called for single item (or a limited set of items) per\\r\\nrequest. While this works well when the API needs to be called from an online\\r\\napplication, the approach breaks down in situations when the API has to be\\r\\ncalled in bulk. For example, during an online sign-up process an address\\r\\nvalidation API can be called for the particular address of the user. But, say in\\r\\na health care analytics application, where addresses of thousands of doctors,\\r\\nwhich already exist in a database or were obtained as part of a bulk load from\\r\\nan external source, have to be verified, this approach will not work. Because of\\r\\nthe “single item per request” design of the API, you’d have to call the API\\r\\nthousands of times.\\r\\n\\r\\nCalling data service APIs in sequence — Processing Time = (# of Records)*(API\\r\\nresponse time)\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nThe above pseudo code snippet shows how calling a target REST API service is\\r\\nhandled in a sequential manner. You must first load the list of parameter values\\r\\nfrom a file or table in the memory. Next run a loop. In the loop, the target\\r\\nREST API has to be called for each set of parameter values. From the response\\r\\nreturned by each call the output must be extracted. The output is typically\\r\\npopulated in a complex object like JSON, XML, etc. Next, the necessary part of\\r\\nthe output has to be added to a result array or collection. For that, you must\\r\\nknow the schema of the result beforehand so that you can process the result\\r\\naccordingly. Finally, you can filter, exploring, aggregating data from the\\r\\nresult array or collection. For all of these steps, you have to use\\r\\nlanguage-specific complex code.\\r\\n\\r\\nAlternatively, you could use a programming language-specific library related to\\r\\nmulti-processing/multi-threading that can parallelize the call to the API.\\r\\nHowever, with that approach the parallelization achieved from a single machine\\r\\nwould be minuscule — limited to the number of cores of the machine. Consider, a\\r\\ncase where someone is trying to get personality insights from tweets or Facebook\\r\\ncomments using a Natural Language Processing service. The tweets and comments\\r\\ncan be in tens to hundreds of thousands. So, using a single machine could take a\\r\\nnumber of hours to get the result. Hence, the approach should be to use a\\r\\ndistributed processing framework to make the API calls parallelized using\\r\\nmultiple cores of multiple machines with the least coding effort. Though it is\\r\\npossible to get distributed computing libraries or frameworks to achieve the\\r\\nsame in some programming languages like Java, C++ etc., they require a\\r\\nreasonable amount of coding and setup to achieve the same result. Achieving this\\r\\nin popular data science languages, like R or Python is actually more difficult\\r\\nas they are originally designed to run in single threaded/single machine\\r\\nenvironment.\\r\\n\\r\\nHere enters distributed computing frameworks like Apache Spark ( https://spark.apache.org/ ). REST APIs are inherently conducive to parallelization as each call to the\\r\\nAPI is completely independent of any other call to the same API. This fact, in\\r\\nconjunction with the parallel computing capability of Spark, can be leveraged to\\r\\ncreate a solution that solves the problem by delegating the API call to Spark’s\\r\\nparallel workers. Under this approach, one can package a specification for how\\r\\nto call the API along with the input data, and pass that to Spark to divide the\\r\\neffort among its workers (and tasks). The output can be assembled in set-level\\r\\nabstractions supported by Spark (like Dataframes or Datasets ) and passed back to the calling program. This approach not only helps you turn a\\r\\nsequential execution into a parallel one with the least coding effort, but also\\r\\nmakes it much easier to analyze and transform the returned result with an easier\\r\\ndata abstraction model to work with.\\r\\n\\r\\nThe performance benefit you get is tremendous in this approach. This turns a\\r\\nproblem that takes incremental time for computation (that increases linearly\\r\\nwith the number of records to process), to one that is much more efficient and\\r\\nscales linearly on a much lower slope — number of records to process divided by\\r\\nthe number of cores available to process them. Theoretically, one can make the\\r\\nprocess constant time by having enough cores to process ALL of the records at\\r\\nonce.\\r\\n\\r\\nTo enable the benefits of using Spark to call REST APIs, we are introducing a\\r\\ncustom data source for Spark, namely REST Data Source. It has been built by\\r\\nextending Spark’s Data Source API. This helps in delegating calls to the target\\r\\nREST API to a Spark level Task for each set of input parameter values/record.\\r\\nThis also enables the results from multiple API calls to be returned as one\\r\\nSpark Dataframe. The REST Data Source expects the input to be in the format of a\\r\\nSpark Temporary table. The results from the API calls are returned in a single\\r\\nDataframe of Rows including the input parameters in their corresponding column\\r\\nnames, as well as the output from the REST call in a structure matching that of\\r\\nthe target API’s response. You can check the schema of this Dataframe, and\\r\\naccess the result as necessary using Spark SQL.\\r\\n\\r\\nThe architecture of REST Data SourceThe above figure shows how REST Data Source works.\\r\\n\\r\\n 1. You first read different sets of parameter values (that have to be sent to\\r\\n    target REST API) from a file/table to a Spark Dataframe (say Input Data\\r\\n    Frame).\\r\\n 2. Then the Input Data Frame is passed to the REST Data Source.\\r\\n 3. The REST Data Source returns the results to another Dataframe, say Result\\r\\n    Data Frame.\\r\\n 4. Now you can use Spark SQL to explore, aggregate, and filter the result using\\r\\n    the Result Data Frame.\\r\\n\\r\\nREST Data Source internally calls the target REST API in parallel by executing\\r\\nmultiple tasks spawned by multiple worker processes running in different\\r\\nmachines. Each task is responsible for calling the target REST API Service for a\\r\\npart of the input (part of sets of parameter values).\\r\\n\\r\\nThe code snippet below demonstrates how to use REST Data Source in Python to get\\r\\nresults from Socrata Data Service (SODA API) for multiple sets of parameter\\r\\nvalues by calling the appropriate REST API in parallel.\\r\\n\\r\\nA sample code snippet showing use of REST Data Source to call REST API in\\r\\nparallelYou can configure the REST Data Source for different extent of parallelization.\\r\\nDepending on the volume of input sets of parameter values to be processed and\\r\\nthroughput supported by the target REST API server, you can pass the number of\\r\\npartitions to be used, and that can limit or extend the level of parallelization\\r\\nas needed. You can use this framework in all programming languages supported by\\r\\nSpark — Python, Scala, R, or Java — without any additional coding specific to\\r\\nthat programming language. Last, but not the least, you can also use this\\r\\nframework to ensure that the target API is called only once for a given set of\\r\\nparameter values. In this way you can avoid calling the target REST API multiple\\r\\ntimes for same set of parameter values. This is especially useful when you must\\r\\npay for the REST API being called or there is a limit per day for the same.\\r\\n\\r\\nSee \\r\\nhttps://github.com/sourav-mazumder/Data-Science-Extensions/tree/master/spark-datasource-rest for details of the REST Data Source.\\r\\n\\r\\nYou can also refer to this notebook \\r\\nhttps://dataplatform.ibm.com/analytics/notebooks/ae63f056-e267-443e-bfc0-b9331f51d68a/view?access_token=0ec63c6e031aa57d065a4e1c4b71733729db43b1490c331a44323cce28725b7d for an example of how to use the REST Data Source.\\r\\n\\r\\nSign up for a free Data Science Experience account ( https://datascience.ibm.com/ ) to try out this technique on a Spark cluster.\\r\\n\\r\\n * Big Data\\r\\n * Spark\\r\\n * Artificial Intelligence\\r\\n * Data Science\\r\\n * Rest Api\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n21 Blocked Unblock Follow FollowingSOURAV MAZUMD...   \n",
      "761  Homepage Follow Sign in Get started Homepage * Home\\r\\n * Data Science Experience\\r\\n * Data Catalog\\r\\n * IBM Data Refinery\\r\\n * \\r\\n * Watson Data Platform\\r\\n * \\r\\n\\r\\nSourav Mazumder Blocked Unblock Follow Following Nov 27\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nUSING APACHE SPARK AS A PARALLEL PROCESSING FRAMEWORK FOR ACCESSING REST BASED\\r\\nDATA SERVICES\\r\\nToday’s world of data science leverages data from various sources. Commonly,\\r\\nthese sources are Hadoop File System, Enterprise Data Warehouse, Relational\\r\\nDatabase systems, Enterprise file systems, etc. The data from these sources are\\r\\naccessed in bulk using connectors specific to the underlying technology and\\r\\noptimized for accessing large volume of data.\\r\\n\\r\\nHowever, many a times, a data science exploration/modeling exercise also needs\\r\\nto access data from sources that support only API-based data access. These\\r\\nAPI-based data sources/data services can be of various types. For example:\\r\\n\\r\\n * Data services (external or internal), which can provide curated/enriched data\\r\\n   in record-by-record manner.\\r\\n * Validation services for verifying the data using an API. For example Address\\r\\n   validation.\\r\\n * Machine learning/AI services, which provide prediction, recommendations, and\\r\\n   insights based on a single input record.\\r\\n * Service from internal systems (like CRM, MDM, etc.) of the organization,\\r\\n   which supports data access through API only in record-by-record manner.\\r\\n * And many more …\\r\\n\\r\\nThese API-based data services are commonly implemented using REST architectural\\r\\nstyle ( https://en.wikipedia.org/wiki/Representational_state_transfer ) and are designed to be called for single item (or a limited set of items) per\\r\\nrequest. While this works well when the API needs to be called from an online\\r\\napplication, the approach breaks down in situations when the API has to be\\r\\ncalled in bulk. For example, during an online sign-up process an address\\r\\nvalidation API can be called for the particular address of the user. But, say in\\r\\na health care analytics application, where addresses of thousands of doctors,\\r\\nwhich already exist in a database or were obtained as part of a bulk load from\\r\\nan external source, have to be verified, this approach will not work. Because of\\r\\nthe “single item per request” design of the API, you’d have to call the API\\r\\nthousands of times.\\r\\n\\r\\nCalling data service APIs in sequence — Processing Time = (# of Records)*(API\\r\\nresponse time)\\r\\n--------------------------------------------------------------------------------\\r\\n\\r\\nThe above pseudo code snippet shows how calling a target REST API service is\\r\\nhandled in a sequential manner. You must first load the list of parameter values\\r\\nfrom a file or table in the memory. Next run a loop. In the loop, the target\\r\\nREST API has to be called for each set of parameter values. From the response\\r\\nreturned by each call the output must be extracted. The output is typically\\r\\npopulated in a complex object like JSON, XML, etc. Next, the necessary part of\\r\\nthe output has to be added to a result array or collection. For that, you must\\r\\nknow the schema of the result beforehand so that you can process the result\\r\\naccordingly. Finally, you can filter, exploring, aggregating data from the\\r\\nresult array or collection. For all of these steps, you have to use\\r\\nlanguage-specific complex code.\\r\\n\\r\\nAlternatively, you could use a programming language-specific library related to\\r\\nmulti-processing/multi-threading that can parallelize the call to the API.\\r\\nHowever, with that approach the parallelization achieved from a single machine\\r\\nwould be minuscule — limited to the number of cores of the machine. Consider, a\\r\\ncase where someone is trying to get personality insights from tweets or Facebook\\r\\ncomments using a Natural Language Processing service. The tweets and comments\\r\\ncan be in tens to hundreds of thousands. So, using a single machine could take a\\r\\nnumber of hours to get the result. Hence, the approach should be to use a\\r\\ndistributed processing framework to make the API calls parallelized using\\r\\nmultiple cores of multiple machines with the least coding effort. Though it is\\r\\npossible to get distributed computing libraries or frameworks to achieve the\\r\\nsame in some programming languages like Java, C++ etc., they require a\\r\\nreasonable amount of coding and setup to achieve the same result. Achieving this\\r\\nin popular data science languages, like R or Python is actually more difficult\\r\\nas they are originally designed to run in single threaded/single machine\\r\\nenvironment.\\r\\n\\r\\nHere enters distributed computing frameworks like Apache Spark ( https://spark.apache.org/ ). REST APIs are inherently conducive to parallelization as each call to the\\r\\nAPI is completely independent of any other call to the same API. This fact, in\\r\\nconjunction with the parallel computing capability of Spark, can be leveraged to\\r\\ncreate a solution that solves the problem by delegating the API call to Spark’s\\r\\nparallel workers. Under this approach, one can package a specification for how\\r\\nto call the API along with the input data, and pass that to Spark to divide the\\r\\neffort among its workers (and tasks). The output can be assembled in set-level\\r\\nabstractions supported by Spark (like dataframes or data sets ) and passed back to the calling program. This approach not only helps you turn a\\r\\nsequential execution into a parallel one with the least coding effort, but also\\r\\nmakes it much easier to analyze and transform the returned result with an easier\\r\\ndata abstraction model to work with.\\r\\n\\r\\nThe performance benefit you gets is tremendous in this approach. This turns a\\r\\nproblem that takes incremental time for computation (that increases linearly\\r\\nwith the number of records to process), to one that is much more efficient and\\r\\nscales linearly on a much lower slope — number of records to process divided by\\r\\nthe number of cores available to process them. Theoretically, one can make the\\r\\nprocess constant time by having enough cores to process ALL of the records at\\r\\nonce.\\r\\n\\r\\nTo enable the benefits of using Spark to call REST APIs, we are introducing a\\r\\ncustom data source for Spark, namely REST Data Source. It has been built by\\r\\nextending Spark’s Data Source API. This helps in delegating calls to the target\\r\\nREST API to a Spark level Task for each set of input parameter values/record.\\r\\nThis also enables the results from multiple API calls to be returned as one\\r\\nSpark Dataframe. The REST Data Source expects the input to be in the format of a\\r\\nSpark Temporary table. The results from the API calls are returned in a single\\r\\nDataframe of Rows including the input parameters in their corresponding column\\r\\nnames, as well as the output from the REST call in a structure matching that of\\r\\nthe target API’s response. You can check the schema of this Dataframe, and\\r\\naccess the result as necessary using Spark SQL.\\r\\n\\r\\nThe architecture of REST Data SourceThe above figure shows how REST Data Source works.\\r\\n\\r\\n 1. You first read different sets of parameter values (that have to be sent to\\r\\n    target REST API) from a file/table to a Spark Dataframe (say Input Data\\r\\n    Frame).\\r\\n 2. Then the Input Data Frame is passed to the REST Data Source.\\r\\n 3. The REST Data Source returns the results to another Dataframe, say Result\\r\\n    Data Frame.\\r\\n 4. Now you can use Spark SQL to explore, aggregate, and filter the result using\\r\\n    the Result Data Frame.\\r\\n\\r\\nREST Data Source internally calls the target REST API in parallel by executing\\r\\nmultiple tasks spawned by multiple worker processes running in different\\r\\nmachines. Each task is responsible for calling the target REST API Service for a\\r\\npart of the input (part of sets of parameter values).\\r\\n\\r\\nThe code snippet below demonstrates how to use REST Data Source in Python to get\\r\\nresults from Socrata Data Service (SODA API) for multiple sets of parameter\\r\\nvalues by calling the appropriate REST API in parallel.\\r\\n\\r\\nA sample code snippet showing use of REST Data Source to call REST API in\\r\\nparallelYou can configure the REST Data Source for different levels of parallelization.\\r\\nDepending on the volume of input sets of parameter values to be processed and\\r\\nthroughput supported by the target REST API server, you can pass the number of\\r\\npartitions to be used, and that can limit or extend the level of parallelization\\r\\nas needed. You can use this framework in all programming languages supported by\\r\\nSpark — Python, Scala, R, or Java — without any additional coding specific to\\r\\nthat programming language. Last, but not the least, you can also use this\\r\\nframework to ensure that the target API is called only once for a given set of\\r\\nparameter values. In this way you can avoid calling the target REST API multiple\\r\\ntimes for same set of parameter values. This is especially useful when you must\\r\\npay for the REST API being called or there is a limit per day for the same.\\r\\n\\r\\nSee \\r\\nhttps://github.com/sourav-mazumder/Data-Science-Extensions/tree/master/spark-datasource-rest for details of the REST Data Source. Also see this notebook \\r\\nhttps://dataplatform.ibm.com/analytics/notebooks/ae63f056-e267-443e-bfc0-b9331f51d68a/view?access_token=0ec63c6e031aa57d065a4e1c4b71733729db43b1490c331a44323cce28725b7d for an example of how to use the REST Data Source.\\r\\n\\r\\n * Big Data\\r\\n * Spark\\r\\n * Artificial Intelligence\\r\\n * Data Science\\r\\n * Rest Api\\r\\n\\r\\nOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.\\r\\n\\r\\n9 Blocked Unblock Follow FollowingSOURAV MAZUMDER\\r\\nMedium member since Nov 2017 FollowIBM WATSON DATA PLATFORM\\r\\nBuild smarter applications and quickly visualize, share, and gain insights\\r\\n...   \n",
      "\n",
      "                                                                                                                                                                                              doc_description  \\\n",
      "399  Today’s world of data science leverages data from various sources. Commonly, these sources are Hadoop File System, Enterprise Data Warehouse, Relational Database systems, Enterprise file systems, etc…   \n",
      "761  Today’s world of data science leverages data from various sources. Commonly, these sources are Hadoop File System, Enterprise Data Warehouse, Relational Database systems, Enterprise file systems, etc…   \n",
      "\n",
      "                                                                                    doc_full_name  \\\n",
      "399  Using Apache Spark as a parallel processing framework for accessing REST based data services   \n",
      "761  Using Apache Spark as a parallel processing framework for accessing REST based data services   \n",
      "\n",
      "    doc_status  article_id  \n",
      "399       Live         398  \n",
      "761       Live         398  \n",
      "-------------\n",
      "577\n",
      "-------------\n",
      "                                                                                                                                                                                                                                                                            doc_body  \\\n",
      "578  This video shows you how to construct queries to access the primary index through the API.Visit http://www.cloudant.com/sign-up to sign up for a free Cloudant account. Find more videos and tutorials in the Cloudant Learning Center: http://www.cloudant.com/learning-center   \n",
      "970                                                                                                          This video shows you how to construct queries to access the primary index through the API.Visit http://www.cloudant.com/sign-up to sign up for a free Cloudant account.   \n",
      "\n",
      "                                                                                       doc_description  \\\n",
      "578  This video shows you how to construct queries to access the primary index through Cloudant's API.   \n",
      "970          This video shows you how to construct queries to access the primary index through the API   \n",
      "\n",
      "             doc_full_name doc_status  article_id  \n",
      "578  Use the Primary Index       Live         577  \n",
      "970  Use the Primary Index       Live         577  \n"
     ]
    }
   ],
   "source": [
    "# Find and explore duplicate articles\n",
    "pd.set_option('display.max_colwidth',10000) # set the amount text per column which is shown\n",
    "for ids in article_multi_val['article_id']:\n",
    "    print('-------------')\n",
    "    print(ids)\n",
    "    print('-------------')\n",
    "    print(df_content[df_content['article_id']==ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicates can be deleted. The reason therefore is that this can cause problems with the matrix factorization. For example, if the same user interact or rate the same article differently then the latent vectors can be optimized in a wrong direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all all rows with duplicates in 'article_id'\n",
    "df_content = df_content.drop_duplicates(subset='article_id', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values) <br>\n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles = len(dict_multi_val['article_id'].values())# The number of unique articles that have at least one interaction\n",
    "total_articles = len(df_content['article_id'].unique())# The number of unique articles on the IBM platform\n",
    "user_article_interactions = df.shape[0]# The number of user-article interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id     0\n",
       "title          0\n",
       "email         17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check nan values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the nan values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = len(df['email'].unique())# The number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_viewed_article_id = str(max(dict_multi_val['article_id'], key=lambda k: dict_multi_val['article_id'][k]))# The most viewed article in the dataset as a string with one value following the decimal \n",
    "max_views = max(dict_multi_val['article_id'].values())# The most viewed article in the dataset was viewed how many times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier data analysis and experimentation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  \\\n",
       "0      1430.0   \n",
       "1      1314.0   \n",
       "2      1429.0   \n",
       "3      1338.0   \n",
       "4      1276.0   \n",
       "\n",
       "                                                                              title  \\\n",
       "0  using pixiedust for fast, flexible, and easier data analysis and experimentation   \n",
       "1                                      healthcare python streaming application demo   \n",
       "2                                        use deep learning for image classification   \n",
       "3                                         ml optimization using cognitive assistant   \n",
       "4                                         deploy your python model as a restful api   \n",
       "\n",
       "   user_id  \n",
       "0        1  \n",
       "1        2  \n",
       "2        3  \n",
       "3        4  \n",
       "4        5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## No need to change the code here - this will be helpful for later parts of the notebook\n",
    "# Run this cell to map the user email to a user_id column and remove the email column\n",
    "\n",
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded\n",
    "\n",
    "# show header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column 'article_id' to string\n",
    "df['article_id'] = df['article_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you have everything right here! Nice job!\n"
     ]
    }
   ],
   "source": [
    "## If you stored all your results in the variable names above, \n",
    "## you shouldn't need to change anything in this cell\n",
    "\n",
    "sol_1_dict = {\n",
    "    '`50% of individuals have _____ or fewer interactions.`': median_val,\n",
    "    '`The total number of user-article interactions in the dataset is ______.`': user_article_interactions,\n",
    "    '`The maximum number of user-article interactions by any 1 user is ______.`': max_views_by_user,\n",
    "    '`The most viewed article in the dataset was viewed _____ times.`': max_views,\n",
    "    '`The article_id of the most viewed article is ______.`': most_viewed_article_id,\n",
    "    '`The number of unique articles that have at least 1 rating ______.`': unique_articles,\n",
    "    '`The number of unique users in the dataset is ______`': unique_users,\n",
    "    '`The number of unique articles on the IBM platform`': total_articles\n",
    "}\n",
    "\n",
    "# Test your dictionary against the solution\n",
    "t.sol_1_test(sol_1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "Unlike in the earlier lessons, we don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Fill in the function below to return the **n** top articles ordered with most interactions as the top. Test your function using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    dict_count = {}\n",
    "    for unique_value in df['article_id'].unique():\n",
    "        if len(df[df['article_id']==unique_value].index) > 0:\n",
    "            dict_count[unique_value] = len(df[df['article_id']==unique_value].index)      \n",
    "    top_articles_ids = list(dict(sorted(dict_count.items(), key=lambda item: item[1], reverse=True)).keys())[:n]\n",
    "    \n",
    "    top_articles = []\n",
    "    for article_id in top_articles_ids:\n",
    "        top_articles.append(list(df[df['article_id']==article_id]['title'])[0])\n",
    "    \n",
    "    return top_articles # Return the top article titles from df (not df_content)\n",
    "\n",
    "def get_top_article_ids(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    dict_count = {}\n",
    "    for unique_value in df['article_id'].unique():\n",
    "        if len(df[df['article_id']==unique_value].index) > 0:\n",
    "            dict_count[unique_value] = len(df[df['article_id']==unique_value].index)      \n",
    "    top_articles = list(dict(sorted(dict_count.items(), key=lambda item: item[1], reverse=True)).keys())[:n]\n",
    "    \n",
    "    return top_articles # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use deep learning for image classification', 'insights from new york car accident reports', 'visualize car data with brunel', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'healthcare python streaming application demo', 'finding optimal locations of new store using decision optimization', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model']\n",
      "['1429.0', '1330.0', '1431.0', '1427.0', '1364.0', '1314.0', '1293.0', '1170.0', '1162.0', '1304.0']\n"
     ]
    }
   ],
   "source": [
    "print(get_top_articles(10))\n",
    "print(get_top_article_ids(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your top_5 looks like the solution list! Nice job.\n",
      "Your top_10 looks like the solution list! Nice job.\n",
      "Your top_20 looks like the solution list! Nice job.\n"
     ]
    }
   ],
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "t.sol_2_test(get_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Use the function below to reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "    # Fill in the function here\n",
    "    \n",
    "    user_item = df.pivot_table(values='title', index='user_id', columns='article_id', aggfunc=lambda x: 1, fill_value=0)\n",
    "    \n",
    "    return user_item # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have passed our quick tests!  Please proceed!\n"
     ]
    }
   ],
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5148, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Complete the function below which should take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar).  The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. \n",
    "\n",
    "Use the tests to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    # compute similarity of each user to the provided user\n",
    "    array_user1 = np.array(user_item.loc[user_id])\n",
    "    \n",
    "    df_user_sim = pd.DataFrame(columns=['user1', 'user2', 'similarity'])\n",
    "    \n",
    "    for index, user_id_matrix in enumerate(user_item.index):\n",
    "        if user_id_matrix == user_id:\n",
    "            next\n",
    "        else:\n",
    "            array_user2 = np.array(user_item.loc[user_id_matrix])\n",
    "            dot_prod = np.dot(array_user1, array_user2)\n",
    "            df_user_sim.loc[index, 'user2'] = user_id_matrix\n",
    "            df_user_sim.loc[index, 'user1'] = user_id\n",
    "            df_user_sim.loc[index, 'similarity'] = dot_prod\n",
    "    \n",
    "    # sort by similarity\n",
    "    # create list of just the ids\n",
    "    most_similar_users_df = df_user_sim.sort_values('similarity', ascending=False)\n",
    "    \n",
    "    most_similar_users = list(most_similar_users_df['user2'])\n",
    "       \n",
    "    return most_similar_users# return a list of the users in order from most to least similar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most similar users to user 1 are: [3932.0, 3781.0, 23.0, 4458.0, 203.0, 3869.0, 131.0, 4200.0, 46.0, 3696.0]\n",
      "The 5 most similar users to user 3933 are: [122.0, 3263.0, 126.0, 3171.0, 3781.0]\n",
      "The 3 most similar users to user 46 are: [4200.0, 3781.0, 23.0]\n"
     ]
    }
   ],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now that you have a function that provides the most similar users to each user, you will want to use these users to find articles you can recommend.  Complete the functions below to return the articles you would recommend to each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    # Your code here\n",
    "    article_names = list(df.loc[df['article_id'].isin(article_ids), 'title'].unique())\n",
    "    \n",
    "    return article_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    # Your code here\n",
    "    article_ids = list(user_item.loc[user_id, user_item.loc[user_id] == 1].index)\n",
    "    \n",
    "    article_names = get_article_names(article_ids, df=df)\n",
    "    \n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Your code here\n",
    "    article_ids_user2, article_names_user1 = get_user_articles(user_id, user_item=user_item)\n",
    "    sim_user = find_similar_users(user_id, user_item=user_item)\n",
    "    \n",
    "    recs = []\n",
    "    \n",
    "    for user in sim_user:\n",
    "        if len(recs_id) <= m:\n",
    "            article_ids_user2, article_names_user2 = get_user_articles(user, user_item=user_item)\n",
    "            recs = recs + list(set(article_ids_user2).difference(set(article_ids_user2)))\n",
    "    \n",
    "    \n",
    "    return recs[:m] # return your recommendations for this user_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, you passed all of our tests!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Test your functions here - No need to change this code - just run this cell\n",
    "assert set(get_article_names(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names(['1320.0', '232.0', '844.0'])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "#assert set(get_user_articles(20)[0]) == set(['1320.0', '232.0', '844.0'])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    \n",
    "    # compute similarity of each user to the provided user\n",
    "    array_user1 = np.array(user_item.loc[user_id])\n",
    "    \n",
    "    neighbors_df = pd.DataFrame(columns=['neighbor_id', 'similarity', 'num_interactions'])\n",
    "    \n",
    "    for index, user_id_matrix in enumerate(user_item.index):\n",
    "        if user_id_matrix == user_id:\n",
    "            next\n",
    "        else:\n",
    "            array_user2 = np.array(user_item.loc[user_id_matrix])\n",
    "            dot_prod = np.dot(array_user1, array_user2)\n",
    "            neighbors_df.loc[index, 'neighbor_id'] = user_id_matrix\n",
    "            neighbors_df.loc[index, 'similarity'] = dot_prod\n",
    "            neighbors_df.loc[index, 'num_interactions'] = len(df[df['user_id']==user_id_matrix].index)\n",
    "    \n",
    "    # sort by similarity\n",
    "    # create list of just the ids\n",
    "    neighbors_df = neighbors_df.sort_values(['similarity','num_interactions'] , ascending=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return neighbors_df # Return the dataframe specified in the doc_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    article_ids_user1, article_names_user1 = get_user_articles(user_id, user_item=user_item)\n",
    "    sim_user = list(get_top_sorted_users(user_id, df=df, user_item=user_item)['neighbor_id'])\n",
    "\n",
    "    recs = []\n",
    "    rec_names = []\n",
    "    for user in sim_user:\n",
    "        if len(recs) <= m:\n",
    "            article_ids_user2, article_names_user2 = get_user_articles(user, user_item=user_item)\n",
    "            new_articles = list(set(article_ids_user2).difference(set(article_ids_user1)))\n",
    "            sorted_df = get_top_sorted_article(new_articles, df=df)\n",
    "            sorted_new_article_id = list(sorted_df['article_id'])  \n",
    "            sorted_new_article_title = list(sorted_df['title'])\n",
    "            recs = recs + sorted_new_article_id\n",
    "            rec_names = rec_names + sorted_new_article_title\n",
    "    \n",
    "    recs = recs[:m]\n",
    "    rec_names = rec_names[:m]\n",
    "    return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_article(article_list, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_list - list of artilce ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    top_sorted_df - (pandas dataframe) a dataframe with:\n",
    "                    artilce_id- sorted article ids on num_interactions\n",
    "                    title - article title\n",
    "                    num_interactions - the viewed number by the user\n",
    "     \n",
    "    '''\n",
    "    \n",
    "    top_sorted_df = pd.DataFrame(columns=['article_id', 'title', 'num_interactions'])\n",
    "    \n",
    "    filtered_df = df[df['article_id'].isin(article_list)]\n",
    "    \n",
    "    for index, article_id in enumerate(article_list):\n",
    "        view_freq  = len(filtered_df[filtered_df['article_id'] == article_id])\n",
    "        top_sorted_df.loc[index, 'article_id'] = article_id\n",
    "        top_sorted_df.loc[index, 'title'] = filtered_df[filtered_df['article_id'] == article_id]['title'].iloc[0]\n",
    "        top_sorted_df.loc[index, 'num_interactions'] = view_freq\n",
    "    \n",
    "\n",
    "    top_sorted_df = top_sorted_df.sort_values('num_interactions' , ascending=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return top_sorted_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 recommendations for user 20 are the following article ids:\n",
      "['1330.0', '1427.0', '1364.0', '1170.0', '1162.0', '1304.0', '1351.0', '1160.0', '1354.0', '1368.0']\n",
      "\n",
      "The top 10 recommendations for user 20 are the following article names:\n",
      "['insights from new york car accident reports', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model', 'model bike sharing data with spss', 'analyze accident reports on amazon emr spark', 'movie recommender system with spark machine learning', 'putting a human face on machine learning']\n"
     ]
    }
   ],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tests with a dictionary of results\n",
    "\n",
    "user1_most_sim = get_top_sorted_users(1, df=df, user_item=user_item)['neighbor_id'].iloc[0]# Find the user that is most similar to user 1 \n",
    "user131_10th_sim = get_top_sorted_users(131, df=df, user_item=user_item)['neighbor_id'].iloc[9]# Find the 10th most similar user to user 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3932.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1_most_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user131_10th_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-bd0702018110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msol_5_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msol_5_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/home/project_tests.py\u001b[0m in \u001b[0;36msol_5_test\u001b[0;34m(sol_5_dict)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msol_5_dict_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msol_5_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msol_5_dict_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Oops!  Looks like there is a mistake with the {} key in your dictionary.  The answer should be {}.  Try again.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` If we were given a new user, which of the above functions would you be able to use to make recommendations?  Explain.  Can you think of a better way we might make recommendations?  Use the cell below to explain a better method for new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7.` Using your existing functions, provide the top 10 recommended articles you would provide for the a new user below.  You can test your function against our thoughts to make sure we are all on the same page with how we might make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_user = '0.0'\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "new_user_recs = # Your recommendations here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert set(new_user_recs) == set(['1314.0','1429.0','1293.0','1427.0','1162.0','1364.0','1304.0','1170.0','1431.0','1330.0']), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, you will build use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform.\n",
    "\n",
    "`1.` You should have already created a **user_item** matrix above in **question 1** of **Part III** above.  This first question here will just require that you run the cells to get things set up for the rest of **Part V** of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the matrix here\n",
    "user_item_matrix = pd.read_pickle('user_item_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick look at the matrix\n",
    "user_item_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` In this situation, you can use Singular Value Decomposition from [numpy](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) on the user-item matrix.  Use the cell to perform SVD, and explain why this is different than in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform SVD on the User-Item Matrix Here\n",
    "\n",
    "u, s, vt = # use the built in to get the three matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, you can see that as the number of latent features increases, we obtain a lower error rate on making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how the accuracy improves as we increase the number of latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_matrix, user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations.  Instead, we might split our dataset into a training and test set of data, as shown in the cell below.  \n",
    "\n",
    "Use the code from question 3 to understand the impact on accuracy of the training and test sets of data with different numbers of latent features. Using the split below: \n",
    "\n",
    "* How many users can we make predictions for in the test set?  \n",
    "* How many users are we not able to make predictions for because of the cold start problem?\n",
    "* How many articles can we make predictions for in the test set?  \n",
    "* How many articles are we not able to make predictions for because of the cold start problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)\n",
    "\n",
    "def create_test_and_train_user_item(df_train, df_test):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item_train - a user-item matrix of the training dataframe \n",
    "                      (unique users for each row and unique articles for each column)\n",
    "    user_item_test - a user-item matrix of the testing dataframe \n",
    "                    (unique users for each row and unique articles for each column)\n",
    "    test_idx - all of the test user ids\n",
    "    test_arts - all of the test article ids\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    return user_item_train, user_item_test, test_idx, test_arts\n",
    "\n",
    "user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the values in the dictionary below\n",
    "a = 662 \n",
    "b = 574 \n",
    "c = 20 \n",
    "d = 0 \n",
    "\n",
    "\n",
    "sol_4_dict = {\n",
    "    'How many users can we make predictions for in the test set?': # letter here, \n",
    "    'How many users in the test set are we not able to make predictions for because of the cold start problem?': # letter here, \n",
    "    'How many articles can we make predictions for in the test set?': # letter here,\n",
    "    'How many articles in the test set are we not able to make predictions for because of the cold start problem?': # letter here\n",
    "}\n",
    "\n",
    "t.sol_4_test(sol_4_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Now use the **user_item_train** dataset from above to find U, S, and V transpose using SVD. Then find the subset of rows in the **user_item_test** dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. This will require combining what was done in questions `2` - `4`.\n",
    "\n",
    "Use the cells below to explore how well SVD works towards making predictions for recommendations on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit SVD on the user_item_train matrix\n",
    "u_train, s_train, vt_train = # fit svd similar to above then use the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use these cells to see how well you can use the training \n",
    "# decomposition to predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`6.` Use the cell below to comment on the results you found in the previous question. Given the circumstances of your results, discuss what you might do to determine if the recommendations you make with any of the above recommendation systems are an improvement to how users currently find articles? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your response here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='conclusions'></a>\n",
    "### Extras\n",
    "Using your workbook, you could now save your recommendations for each user, develop a class to make new predictions and update your results, and make a flask app to deploy your results.  These tasks are beyond what is required for this project.  However, from what you learned in the lessons, you certainly capable of taking these tasks on to improve upon your work here!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "> Congratulations!  You have reached the end of the Recommendations with IBM project! \n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the [rubric](https://review.udacity.com/#!/rubrics/2322/view). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "\n",
    "## Directions to Submit\n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
